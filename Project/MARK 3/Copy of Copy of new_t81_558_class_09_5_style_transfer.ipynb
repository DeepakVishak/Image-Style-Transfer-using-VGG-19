{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JQw_M1fg18u"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_09_5_style_transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7OGIjDtFg18w"
   },
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "**Module 9: Transfer Learning**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1y7Dvqmwg18w"
   },
   "source": [
    "# Module 9 Material\n",
    "\n",
    "* Part 9.1: Introduction to Keras Transfer Learning [[Video]](https://www.youtube.com/watch?v=AtoeoNwmd7w&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_09_1_keras_transfer.ipynb)\n",
    "* Part 9.2: Keras Transfer Learning for Computer Vision [[Video]](https://www.youtube.com/watch?v=nXcz0V5SfYw&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_09_2_keras_xfer_cv.ipynb)\n",
    "* Part 9.3: Transfer Learning for NLP with Keras [[Video]](https://www.youtube.com/watch?v=PyRsjwLHgAU&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_09_3_transfer_nlp.ipynb)\n",
    "* Part 9.4: Transfer Learning for Facial Feature Recognition [[Video]](https://www.youtube.com/watch?v=uUZg33DfCls&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_09_4_facial_points.ipynb)\n",
    "* **Part 9.5: Transfer Learning for Style Transfer** [[Video]](https://www.youtube.com/watch?v=pLWIaQwkJwU&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_09_5_style_transfer.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXcDobUGg18w"
   },
   "source": [
    "# Google CoLab Instructions\n",
    "\n",
    "The following code ensures that Google CoLab is running the correct version of TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1674987009159,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "fGHx8vRag18x",
    "outputId": "703c0113-038f-4e99-9fd7-0c8278813416"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: not using Google CoLab\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google\n",
      "  Downloading google-3.0.0-py2.py3-none-any.whl (45 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\dexter\\anaconda3\\lib\\site-packages (from google) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\dexter\\anaconda3\\lib\\site-packages (from beautifulsoup4->google) (2.3.1)\n",
      "Installing collected packages: google\n",
      "Successfully installed google-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install google"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rz2ortFxg18x"
   },
   "source": [
    "# Part 9.5: Transfer Learning for Keras Style Transfer\n",
    "\n",
    "In this part, we will implement style transfer. This technique takes two images as input and produces a third. The first image is the base image that we wish to transform. The second image represents the style we want to apply to the source image. Finally, the algorithm renders a third image that emulates the style characterized by the style image. This technique is called style transfer. [[Cite:gatys2016image]](https://openaccess.thecvf.com/content_cvpr_2016/html/Gatys_Image_Style_Transfer_CVPR_2016_paper.html)\n",
    "\n",
    "**Figure 9.STYLE_TRANS: Style Transfer**\n",
    "![Style Transfer](https://data.heatonresearch.com/images/jupyter/style_trans.jpg)\n",
    "\n",
    "I based the code presented in this part on a style transfer example in the Keras documentation created by [Fran√ßois Chollet](https://keras.io/examples/generative/neural_style_transfer/).\n",
    "\n",
    "We begin by uploading two images to Colab. If running this code locally, point these two filenames at the local copies of the images you wish to use.\n",
    "\n",
    "* **base_image_path** - The image to apply the style to.\n",
    "* **style_reference_image_path** - The image whose style we wish to copy.\n",
    "\n",
    "First, we upload the base image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "executionInfo": {
     "elapsed": 44602,
     "status": "ok",
     "timestamp": 1674987053743,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "T4DdBOidw3_3",
    "outputId": "97da5525-01dd-4e32-cca7-3c8758e6b2a2"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# HIDE OUTPUT\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m files\n\u001b[0;32m      5\u001b[0m uploaded \u001b[38;5;241m=\u001b[39m files\u001b[38;5;241m.\u001b[39mupload()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uploaded) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# HIDE OUTPUT\n",
    "import os\n",
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "if len(uploaded) != 1:\n",
    "  print(\"Upload exactly 1 file for source.\")\n",
    "else:\n",
    "  for k, v in uploaded.items():\n",
    "    _, ext = os.path.splitext(k)\n",
    "    os.remove(k)\n",
    "    base_image_path = f\"source{ext}\"\n",
    "    open(base_image_path, 'wb').write(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f32blRjVxHXb"
   },
   "source": [
    "We also, upload the style image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "executionInfo": {
     "elapsed": 15020,
     "status": "ok",
     "timestamp": 1674987068746,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "NeP0MlkKxII2",
    "outputId": "3f5fee7f-6385-4408-a573-421a9765fc21"
   },
   "outputs": [],
   "source": [
    "# HIDE OUTPUT\n",
    "uploaded = files.upload()\n",
    "\n",
    "if len(uploaded) != 1:\n",
    "  print(\"Upload exactly 1 file for target.\")\n",
    "else:\n",
    "  for k, v in uploaded.items():\n",
    "    _, ext = os.path.splitext(k)\n",
    "    os.remove(k)\n",
    "    style_reference_image_path = f\"style{ext}\"\n",
    "    open(style_reference_image_path, 'wb').write(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yk7d64ZxxRxP"
   },
   "source": [
    "The loss function balances three different goals defined by the following three weights. Changing these weights allows you to fine-tune the image generation.\n",
    "\n",
    "* **total_variation_weight** - How much emphasis to place on the visual coherence of nearby pixels.\n",
    "* **style_weight** - How much emphasis to place on emulating the style of the reference image.\n",
    "* **content_weight** - How much emphasis to place on remaining close in appearance to the base image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2480,
     "status": "ok",
     "timestamp": 1674987071215,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "oSy_RZ0kg180"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import vgg19\n",
    "\n",
    "result_prefix = \"generated\"\n",
    "\n",
    "# Weights of the different loss components\n",
    "total_variation_weight = 1e-6\n",
    "style_weight = 1e-6\n",
    "content_weight = 2.5e-8\n",
    "\n",
    "# Dimensions of the generated picture.\n",
    "width, height = keras.preprocessing.image.load_img(base_image_path).size\n",
    "img_nrows = 400\n",
    "img_ncols = int(width * img_nrows / height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uernZ-CyqnXb"
   },
   "source": [
    "We now display the two images we will use, first the base image followed by the style image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1674987071216,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "7g0lJJk6lQiJ",
    "outputId": "e436d408-c8d2-422f-ffdf-93eb9e5c1368"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "print(\"Source Image\")\n",
    "display(Image(base_image_path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1674987071218,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "ALtYZNuNHKXB",
    "outputId": "af68c27d-26ff-4fb4-fa0f-d3a45b496a26"
   },
   "outputs": [],
   "source": [
    "print(\"Style Image\")\n",
    "display(Image(style_reference_image_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5-WQSy1q802"
   },
   "source": [
    "## Image Preprocessing and Postprocessing\n",
    "\n",
    "The preprocess_image function begins by loading the image using Keras. We scale the image to the size specified by img_nrows and img_ncols. The img_to_array  converts the image to a Numpy array, to which we add dimension to account for batching. The dimensions expected by VGG are colors depth, height, width, and batch. Finally, we convert the Numpy array to a tensor.\n",
    "\n",
    "The deprocess_image performs the reverse, transforming the output of the style transfer process back to a regular image. First, we reshape the image to remove the batch dimension. Next, The outputs are moved back into the 0-255 range by adding the mean value of the RGB colors. We must also convert the BGR (blue, green, red) colorspace of VGG to the more standard RGB encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1674987071220,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "PwNHLfZglqCe"
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    # Util function to open, resize and format \n",
    "    # pictures into appropriate tensors\n",
    "    img = keras.preprocessing.image.load_img(\n",
    "        image_path, target_size=(img_nrows, img_ncols)\n",
    "    )\n",
    "    img = keras.preprocessing.image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = vgg19.preprocess_input(img)\n",
    "    return tf.convert_to_tensor(img)\n",
    "\n",
    "\n",
    "def deprocess_image(x):\n",
    "    # Util function to convert a tensor into a valid image\n",
    "    x = x.reshape((img_nrows, img_ncols, 3))\n",
    "    # Remove zero-center by mean pixel\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    # 'BGR'->'RGB'\n",
    "    x = x[:, :, ::-1]\n",
    "    x = np.clip(x, 0, 255).astype(\"uint8\")\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dUBdAeVwrLgC"
   },
   "source": [
    "## Calculating the Style, Content, and Variation Loss\n",
    "\n",
    "Before we see how to calculate the 3-part loss function, I must introduce the Gram matrix's mathematical concept. Figure 9.GRAM demonstrates this concept. \n",
    "\n",
    "**Figure 9.GRAM: The Gram Matrix**\n",
    "![The Gram Matrix](https://data.heatonresearch.com/images/jupyter/gram.jpg)\n",
    "\n",
    "We calculate the Gram matrix by multiplying a matrix by its transpose. To calculate two parts of the loss function, we will take the Gram matrix of the outputs from several convolution layers in the VGG network. To determine both style, and similarity to the original image, we will compare the convolution layer output of VGG rather than directly comparing the image pixels. In the third part of the loss function, we will directly compare pixels near each other.\n",
    "\n",
    "Because we are taking convolution output from several different levels of the VGG network, the Gram matrix provides a means of combining these layers. The Gram matrix of the VGG convolution layers represents the style of the image. We will calculate this style for the original image, the style-reference image, and the final output image as the algorithm generates it. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1674987071221,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "xOcBOgu5lthY"
   },
   "outputs": [],
   "source": [
    "# The gram matrix of an image tensor (feature-wise outer product)\n",
    "def gram_matrix(x):\n",
    "    x = tf.transpose(x, (2, 0, 1))\n",
    "    features = tf.reshape(x, (tf.shape(x)[0], -1))\n",
    "    gram = tf.matmul(features, tf.transpose(features))\n",
    "    return gram\n",
    "\n",
    "\n",
    "# The \"style loss\" is designed to maintain\n",
    "# the style of the reference image in the generated image.\n",
    "# It is based on the gram matrices (which capture style) of\n",
    "# feature maps from the style reference image\n",
    "# and from the generated image\n",
    "def style_loss(style, combination):\n",
    "    S = gram_matrix(style)\n",
    "    C = gram_matrix(combination)\n",
    "    channels = 3\n",
    "    size = img_nrows * img_ncols\n",
    "    return tf.reduce_sum(tf.square(S - C)) /\\\n",
    "      (4.0 * (channels ** 2) * (size ** 2))\n",
    "\n",
    "\n",
    "# An auxiliary loss function\n",
    "# designed to maintain the \"content\" of the\n",
    "# base image in the generated image\n",
    "def content_loss(base, combination):\n",
    "    return tf.reduce_sum(tf.square(combination - base))\n",
    "\n",
    "\n",
    "# The 3rd loss function, total variation loss,\n",
    "# designed to keep the generated image locally coherent\n",
    "def total_variation_loss(x):\n",
    "    a = tf.square(\n",
    "        x[:, : img_nrows - 1, : img_ncols - 1, :] \\\n",
    "          - x[:, 1:, : img_ncols - 1, :]\n",
    "    )\n",
    "    b = tf.square(\n",
    "        x[:, : img_nrows - 1, : img_ncols - 1, :] \\\n",
    "          - x[:, : img_nrows - 1, 1:, :]\n",
    "    )\n",
    "    return tf.reduce_sum(tf.pow(a + b, 1.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWM_2KB4rXvJ"
   },
   "source": [
    "The style_loss function compares how closely the current generated image (combination) matches the style of the reference style image. The Gram matrixes of the style and current generated image are subtracted and normalized to calculate this difference in style. Precisely, it consists in a sum of L2 distances between the Gram matrices of the representations of the base image and the style reference image, extracted from different layers of VGG. The general idea is to capture color/texture information at different spatial scales (fairly large scales, as defined by the depth of the layer considered).\n",
    "\n",
    "The content_loss function compares how closely the current generated image matches the original image. You must subtract Gram matrixes of the original and generated images to calculate this difference. Here we calculate the L2 distance between the base image's VGG features and the generated image's features, keeping the generated image close enough to the original one.\n",
    "\n",
    "Finally, the total_variation_loss function imposes local spatial continuity between the pixels of the generated image, giving it visual coherence.\n",
    "\n",
    "## The VGG Neural Network\n",
    "\n",
    "VGG19 is a convolutional neural network model proposed by K. Simonyan and A. Zisserman. [[Cite:simonyan2014very]](https://arxiv.org/abs/1409.1556) The model achieves 92.7% top-5 test accuracy in ImageNet, a dataset of over 14 million images belonging to 1000 classes. We will transfer the VGG16 weights into our style transfer model. Keras provides functions to load the VGG neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7155,
     "status": "ok",
     "timestamp": 1674987078350,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "zfhvFXQ3lzJi",
    "outputId": "ed45e2ed-caf3-40f8-998f-12c82f52eac0"
   },
   "outputs": [],
   "source": [
    "# HIDE OUTPUT\n",
    "# Build a VGG19 model loaded with pre-trained ImageNet weights\n",
    "model = vgg19.VGG19(weights=\"imagenet\", include_top=False)\n",
    "\n",
    "# Get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n",
    "\n",
    "# Set up a model that returns the activation values for every layer in\n",
    "# VGG19 (as a dict).\n",
    "feature_extractor = keras.Model(inputs=model.inputs, outputs=outputs_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPKN6lZgVFr8"
   },
   "source": [
    "We can now generate the complete loss function. The following images are input to the compute_loss function:\n",
    "\n",
    "* **combination_image** - The current iteration of the generated image.\n",
    "* **base_image** - The starting image.\n",
    "* **style_reference_image** - The image that holds the style to reproduce.\n",
    "\n",
    "The layers specified by style_layer_names indicate which layers should be extracted as features from VGG for each of the three images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1674987078351,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "dfiV1QMXl2iM"
   },
   "outputs": [],
   "source": [
    "# List of layers to use for the style loss.\n",
    "style_layer_names = [\n",
    "    \"block1_conv1\",\n",
    "    \"block2_conv1\",\n",
    "    \"block3_conv1\",\n",
    "    \"block4_conv1\",\n",
    "    \"block5_conv1\",\n",
    "]\n",
    "# The layer to use for the content loss.\n",
    "content_layer_name = \"block5_conv2\"\n",
    "\n",
    "\n",
    "def compute_loss(combination_image, base_image, style_reference_image):\n",
    "    input_tensor = tf.concat(\n",
    "        [base_image, style_reference_image, combination_image], axis=0\n",
    "    )\n",
    "    features = feature_extractor(input_tensor)\n",
    "\n",
    "    # Initialize the loss\n",
    "    loss = tf.zeros(shape=())\n",
    "\n",
    "    # Add content loss\n",
    "    layer_features = features[content_layer_name]\n",
    "    base_image_features = layer_features[0, :, :, :]\n",
    "    combination_features = layer_features[2, :, :, :]\n",
    "    loss = loss + content_weight * content_loss(\n",
    "        base_image_features, combination_features\n",
    "    )\n",
    "    # Add style loss\n",
    "    for layer_name in style_layer_names:\n",
    "        layer_features = features[layer_name]\n",
    "        style_reference_features = layer_features[1, :, :, :]\n",
    "        combination_features = layer_features[2, :, :, :]\n",
    "        sl = style_loss(style_reference_features, combination_features)\n",
    "        loss += (style_weight / len(style_layer_names)) * sl\n",
    "\n",
    "    # Add total variation loss\n",
    "    loss += total_variation_weight * \\\n",
    "      total_variation_loss(combination_image)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HgKsDf8vreS8"
   },
   "source": [
    "## Generating the Style Transferred Image\n",
    "\n",
    "The compute_loss_and_grads function calls the loss function and computes the gradients. The parameters of this model are the actual RGB values of the current iteration of the generated images. These parameters start with the base image, and the algorithm optimizes them to the final rendered image. We are not training a model to perform the transformation; we are training/modifying the image to minimize the loss functions. We utilize gradient tape to allow Keras to modify the image in the same way the neural network training modifies weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1674987078352,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "vAe8PYgFl6mF"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_loss_and_grads(combination_image, \\\n",
    "                  base_image, style_reference_image):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(combination_image, \\\n",
    "                base_image, style_reference_image)\n",
    "    grads = tape.gradient(loss, combination_image)\n",
    "    return loss, grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CqhYMgblZ6Vc"
   },
   "source": [
    "We can now optimize the image according to the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 515666,
     "status": "ok",
     "timestamp": 1674988138865,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "TKnXl-uml-Kd",
    "outputId": "ab8cd1ec-657a-432c-979e-1ca815d96660"
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(\n",
    "    keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=100.0, decay_steps=100, decay_rate=0.96\n",
    "    )\n",
    ")\n",
    "\n",
    "base_image = preprocess_image(base_image_path)\n",
    "style_reference_image = preprocess_image(style_reference_image_path)\n",
    "combination_image = tf.Variable(preprocess_image(base_image_path))\n",
    "\n",
    "iterations = 2000\n",
    "for i in range(1, iterations + 1):\n",
    "    loss, grads = compute_loss_and_grads(\n",
    "        combination_image, base_image, style_reference_image\n",
    "    )\n",
    "    optimizer.apply_gradients([(grads, combination_image)])\n",
    "    if i % 100 == 0:\n",
    "        print(\"Iteration %d: loss=%.2f\" % (i, loss))\n",
    "        img = deprocess_image(combination_image.numpy())\n",
    "        fname = result_prefix + \"_at_iteration_%d.png\" % i\n",
    "        keras.preprocessing.image.save_img(fname, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cybwFciZuMz"
   },
   "source": [
    "We can display the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "executionInfo": {
     "elapsed": 3502,
     "status": "ok",
     "timestamp": 1674988207259,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "7H4NREcemBCX",
    "outputId": "508c0a6f-1179-4251-ed7f-f010b901a2f5"
   },
   "outputs": [],
   "source": [
    "display(Image(result_prefix + \"_at_iteration_2000.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XK6GFkA1HhHK"
   },
   "source": [
    "We can download this image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1674988216976,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "RP3SOtfwHz5r",
    "outputId": "cf2a107b-e865-499a-8fdb-9c28d17bb059"
   },
   "outputs": [],
   "source": [
    "# HIDE OUTPUT\n",
    "from google.colab import files\n",
    "files.download(result_prefix + \"_at_iteration_2000.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZpsPT2gzEzXl"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "name": "Copy of new_t81_558_class_09_5_style_transfer.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_09_5_style_transfer.ipynb",
     "timestamp": 1674990462932
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
